---
layout: post
comments: false
title:  "(started posting on Medium instead)"
excerpt: "Yes I'm still around but, I've started posting on Medium instead of here."
date:   2019-03-28 11:00:00
mathjax: false
---

Natural Language Processing (NLP) has made a major leap in the past decade, both in theory and in practical integration into broadly deployed industry solutions. From interacting with virtual assistants to texting with a flight-booking chatbot to extracting insights from call center interactions to analyze customer satisfaction levels, NLP is everywhere today. In 2018, Deep learning (DL) for NLP finally hit its stride with neural nets outperforming traditional machine learning (ML) methods in a wide range of NLP tasks, and even surpassing human performance in the challenging areas of question answering and machine translation. The field seems poised to continue its advances in using and understanding natural language. However, there are a number of significant challenges when deploying NLP solutions in commercial environments that need to be addressed. Among the top challenges are improving focused learning from a small set of examples and scaling solutions across different domains. A few recent technology advances and practices hold great promise for improving scalability and robustness and embody a shift in how business organizations consume computing resources and deploy NLP applications.

Prior “State-of-the-Art”: BiLSTM Hegemony
In 1997, Sepp Hochreiter and Jürgen Schmidhuber introduced the long short term memory (LSTM), a memory cell unit that efficiently stores information over extended time intervals via Recurrent Neural Networks (RNN). LSTMs have since become a key tool for capturing long-range dependencies for NLP tasks. Two decades later, the bi-directional LSTM DL version (BiLSTM) has driven many of the advances in NLP, powered by neural network (NN) computing resources, word-embedding representations and greater access to useful datasets. As Chris Manning, director of Stanford’s Artificial Intelligence Library, explains:” No matter what the [NLP] task, you throw a BiLSTM at it.” [1]


### TLDR 

**Have a look at my [Medium blog](https://medium.com/@NLP_Architect/).**
